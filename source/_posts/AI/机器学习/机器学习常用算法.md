---
title: 机器学习常用算法
mathjax: true
toc: true
date: 2021-01-04 15:37:37
tags:
categories:
---


## 单变量线性回归 / Linear Regression with One Variable
单变量线性回归模型: $h_\theta(x)=\theta_0+\theta_1x$
代价函数: $J \left( \theta_0, \theta_1 \right) = \frac{1}{2m}\sum\limits_{i=1}^m \left( h_{\theta}(x^{(i)})-y^{(i)} \right)^{2}$

- m代表训练集中实例的数量
- x代表特征/输入变量
- y代表目标变量/输出变量
- (x, y)代表训练集中的实例
- $(x^{(i)},y^{(i)})$ 代表第i个观察实例
- h 代表学习算法的解决方案或函数也称为假设（hypothesis）
- 上面h代表学习算法的解决方案或函数也称为假设（hypothesis）
- 代价函数中的$1 \over 2$是为了在做梯度下降时消掉平方

代价函数也被称作平方误差函数，有时也被称为平方误差代价函数。我们之所以要求出误差的平方和，是因为误差平方代价函数，对于大多数问题，特别是回归问题，都是一个合理的选择。还有其他的代价函数也能很好地发挥作用，但是平方误差代价函数可能是解决回归问题最常用的手段了。

我们的目标是为我们的模型选择合适的参数（parameters）$\theta_0 和 \theta_1$，可以使得建模误差的平方和，即使得代价函数的值最小的模型参数。

> 梯度下降/Gradient Descent

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0, \theta_1)$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合$(\theta_0, \theta_1,...... \theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

梯度下降法的更新规则：${\theta_{j}}:={\theta_{j}}-\alpha \frac{\partial }{\partial {\theta_{j}}}J\left( \theta \right)$

描述：对$\theta$赋值，使得$J(\theta)$按梯度下降最快方向进行，一直迭代下去，最终得到局部最小值，需要注意的是每次迭代时要同步更新$(\theta_0,\theta_1)$，即全部计算出$(\theta_0,\theta_1)$后，再分别赋值，而不是先计算出$\theta_0$，给$\theta_0$赋值，再计算$\theta_1$，然后赋值，这样做会导致我们计算的$\theta_1$是不准确的，准确说是计算代价函数$J(\theta)$对$\theta_j$的导数不准确。
- $\alpha$是学习率（**learning rate**），一般取值为0-1，常用用0.8，它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大。
- $\frac{\partial }{\partial {\theta_{j}}}J\left( \theta \right)$ 是代价函数$J(\theta)$对$\theta_j$的导数。

如果它有正导数，表示单调递增，应用更新规则后，得到的新的${\theta_{1}}$，${\theta_{1}}$减小，相应的代价函数$J()$的值会减小，负导数表示单调递增，相应的会使新的${\theta_{1}}$，${\theta_{1}}$增大，相应的代价函数$J()$的值也会减小。

让我们来看看如果太小或太大会出现什么情况：
- 如果太小了，即我的学习速率太小，结果就是只能这样像小宝宝一样一点点地挪动，去努力接近最低点，这样就需要很多步才能到达最低点，所以如果太小的话，可能会很慢，因为它会一点点挪动，它会需要很多步才能到达全局最低点。
- 如果太大，那么梯度下降法可能会越过最低点，甚至可能无法收敛，下一次迭代又移动了一大步，越过一次，又越过一次，一次次越过最低点，直到你发现实际上离最低点越来越远，所以，如果太大，它会导致无法收敛，甚至发散。

假设你将初始化在局部最低点，在这儿，它已经在一个局部的最优处或局部最低点。结果是局部最优点的导数将等于零，因为它是那条切线的斜率。这意味着你已经在局部最优点，它使得不再改变，也就是新的等于原来的，因此，如果你的参数已经处于局部最低点，那么梯度下降法更新其实什么都没做，它不会改变参数的值。这也解释了为什么即使学习速率保持不变时，梯度下降也可以收敛到局部最低点。

另外，如果我每更新一步梯度下降，随着接近最低点，导数越来越接近零，所以，梯度下降一步后，新的导数会变更小一点，$\theta$更新的幅度就会更小。所以随着梯度下降法的运行，你移动的幅度会自动变得越来越小，直到最终移动幅度非常小，你会发现，在梯度下降法中，当我们接近局部最低点时，梯度下降法会自动采取更小的幅度，这是因为当我们接近局部最低点时，很显然在局部最低时导数等于零，所以当我们接近局部最低时，导数值会自动变得越来越小，所以梯度下降将自动采取较小的幅度，这就是梯度下降的做法。所以实际上没有必要再另外减小$\alpha$。

这就是梯度下降算法，你可以用它来最小化任何代价函数，不只是线性回归中的代价函数。

----

对我们之前的线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：

todo..

我们刚刚使用的算法，有时也称为批量梯度下降，指的是在梯度下降的每一步中，我们都用到了所有的训练样本，在梯度下降中，在计算微分求导项时，我们需要进行求和运算，所以，在每一个单独的梯度下降中，我们最终都要计算这样一个东西，这个项需要对所有个训练样本求和。因此，批量梯度下降法这个名字说明了我们需要考虑所有这一"批"训练样本，而事实上，有时也有其他类型的梯度下降法，不是这种"批量"型的，不考虑整个的训练集，而是每次只关注训练集中的一些小的子集。在后面的课程中，我们也将介绍这些方法。

另外还有一种计算代价函数$J(\theta_0, \theta_1)$最小值的数值解法，不需要迭代，称为正规方程(normal equations)的方法。实际上在数据量较大（大概1000个特征值）的情况下，梯度下降法比正规方程要更适用一些。

## 多变量线性回归(Linear Regression with Multiple Variables)


## 参考资料
> - []()
> - []()
